<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <title>Unsupervised Discovery and Composition of Object Light Fields</title>
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css"
          integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">

    <!-- Custom styles for this template -->
    <link href="web/offcanvas.css" rel="stylesheet">
    <!--    <link rel="icon" href="img/favicon.gif" type="image/gif">-->
</head>

<style>
.section_title {
    text-align:center;

}
</style>

<body>
<div class="jumbotron jumbotron-fluid">
    <div class="container"></div>
    <h2>Unsupervised Discovery and Composition of Object Light Fields</h2>
    <!--<h3></h3>-->
    <hr> 
    <p class="authors">
        <a href="https://cameronosmith.github.io/"> Cameron Smith</a>,
        <a href="https://kovenyu.com/"> Hong-Xing "Koven" Yu,</a>
        <a href="https://zakharos.github.io/"> Sergey Zakharov,</a><br>
        <a href="https://people.csail.mit.edu/fredo/"> Fredo Durand,</a>
        <a href="https://mitibmwatsonailab.mit.edu/people/joshua-tenenbaum/"> Joshua Tenenbaum,</a>
        <a href="https://jiajunwu.com/"> Jiajun Wu,</a>
        <a href="http://www.stanford.edu/~sitzmann/"> Vincent Sitzmann</a>
    </p>
    <div class="btn-group" role="group" aria-label="Top menu">
        <!--<a class="btn btn-primary" href="">Paper</a> -->
        <a class="btn btn-primary" href="https://github.com/cameronosmith/COLF">Code</a>
        <a class="btn btn-primary" href="https://arxiv.org/abs/2205.03923">Paper</a>
        <a class="btn btn-primary" href="https://drive.google.com/drive/folders/14BCRAByUP7SIEBzgDxUROZ_t_vNJXglT?usp=sharing">Data</a>
    </div>
</div>

<div class="container">
    <div class="section">
        <hr>
        <p>
Neural scene representations, both continuous and discrete, have recently emerged as a powerful new paradigm for 3D scene understanding. Recent efforts have tackled unsupervised discovery of object-centric neural scene representations. However, the high cost of ray-marching, exacerbated by the fact that each object representation has to be ray-marched separately, leads to insufficiently sampled radiance fields and thus, noisy renderings, poor framerates, and high memory and time complexity during training and rendering. Here, we propose to represent objects in an object-centric, compositional scene representation as light fields. We propose a novel light field compositor module that enables reconstructing the global light field from a set of object-centric light fields. Dubbed Compositional Object Light Fields (COLF), our method enables unsupervised learning of object-centric neural scene representations, state-of-the-art reconstruction and novel view synthesis performance on standard datasets, and rendering and training speeds at orders of magnitude faster than existing 3D approaches.

        <br>
        <br>
        <b> TL;DR </b> We combine slot attention and light field networks to 
            yield a 3D-aware, object-centric model without prohibitive memory
            constraints.
        </p>
        <img src="web/img/overview_iter3-7.png"
                     style="width:100%; margin-right:-20px; margin-top:20px;">
    </div>

    <div class="section">
        <hr>
    </div>

    <div class="section">
        <h2 class="section_title">Scene Decomposition</h2>
        <p>
        From a single view of a scene, our model infers a 3D description for each object
        and reconstructs the scene from a novel pose. We visualize novel views
        of each scene and its per-object contributions. Per-object contributions
        shown here are masked by their segmentation weight against the background slot. 
        </p>
        <video class="video" autoplay="true" loop="true" autoplay muted>
          <source src="web/mov/rotcomb.mp4">
        </video>
        <hr>
    </div>
    <div class="section">
        <h2 class="section_title">Scene Editing: Cross-Scene Composition </h2>
        <p>
        Below, we demonstrate the advantage of using a light-field decoder in a
        cross-scene composition application, where we combine object latents
        from ten different scenes into a single scene with twenty objects.
        Prior object-centric, 3D-aware models are unable to represent scenes with
        such depth range and number of objects due to prohibitive memory
        constraints. The scene reconstruction and its per-object contributions
        are displayed below.
        </p>


        <video class="video" autoplay="true" loop="true" autoplay muted>
          <source src="web/mov/car.mp4">
        </video>
        <hr>
    </div>

    <div class="section">
        <h2 class="section_title">Scene Editing: Object Manipulation</h2>
        <p>
        We demonstrate a scene editing application of editing the scene on a
        per-object scale. For each scene, its objects are deleted sequentially
        (left), an object is centered (middle), and an object is orbited about
        the centered object (right).
        </p>


        <video class="video" autoplay="true" loop="true" autoplay muted>
          <source src="web/mov/manipcomb.mp4">
        </video>
        <hr>
    </div>
    <div class="section">
        <h2 class="section_title">Paper</h2>
            <div class="list-group">
                <a href="https://arxiv.org/abs/2205.03923" class="list-group-item">
                    <img href="" src="paperthumbnail.png" style="width:100%; margin-right:-20px; margin-top:0px;">
                </a>
        </div>
        <hr>
    </div>
    <div class="section">
    <h2 class="section_title">Bibtex</h2>
    <div class="bibtexsection">
            @article{smith2023colf,
                author = {Smith, Cameron
                          and Yu, Hong-Xing
                          and Zakharov, Sergey
                          and Durand, Fredo
                          and Tenenbaum, Joshua B.
                          and Wu, Jiajun
                          and Sitzmann, Vincent},
                title = {Unsupervised Discovery and Composition
                         of Object Light Fields},
                journal = {Transactions on Machine Learning Research (TMLR)},
                year = {2023}
            }

            </div>
    </div>
    <div class="section">
    </div>

    <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"
            integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj"
            crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
            integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
            crossorigin="anonymous"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.min.js"
            integrity="sha384-OgVRvuATP1z7JjHLkuOU7Xw704+h835Lr+6QL9UvYjZE3Ipu6Tp75j7Bh/kR0JKI"
            crossorigin="anonymous"></script>

</body>

</html>
