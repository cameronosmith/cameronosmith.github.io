<!DOCTYPE html>
<html lang="en">

<script src="bootstrap.js"></script>
<script type="text/javascript" charset="utf-8" src="https://ajax.googleapis.com/ajax/libs/jquery/1.3.2/jquery.min.js"></script> 
<!---
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
--->
<script src="load-mathjax.js" async></script>


<style type="text/css">
body {
    font-family: "Titillium Web", "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
    font-weight: 300;
    font-size: 17px;
    margin-left: auto;
    margin-right: auto;
}

@media screen and (min-width: 980px){
    body {
        width: 980px;
    }
}

.video-row {
display: flex;
flex-wrap: wrap;
justify-content: space-between;
align-items: center;
height: 300px;
}

.video-row video {
width: 50%;
height: 100%;
object-fit: cover;
}




h1 {
    font-weight:300;
    line-height: 1.15em;
}

h2 {
    font-size: 1.75em;
}
a:link,a:visited {
    color: #5364cc;
    text-decoration: none;
}
a:hover {
    color: #208799;
}
h1 {
    text-align: center;
}
h2,h3 {
    text-align: left;
}

h1 {
    font-size: 40px;
    font-weight: 500;
}
h2 {
    font-weight: 400;
    margin: 16px 0px 4px 0px;
}
h3 {
    font-weight: 600;
    margin: 16px 0px 4px 0px;
}

.paper-title {
    padding: 1px 0px 1px 0px;
}
section {
    margin: 32px 0px 32px 0px;
    text-align: justify;
    clear: both;
}
.col-5 {
     width: 20%;
     float: left;
}

.move-down {
    margin-top:0.6cm;
}

.col-4 {
     width: 25%;
     float: left;
}
.col-3 {
     width: 33%;
     float: left;
}
.col-2 {
     width: 50%;
     float: left;
}
.col-1 {
     width: 100%;
     float: left;
}

.author-row, .affil-row {
    font-size: 26px;
}

.author-row-new { 
    text-align: center; 
}

.author-row-new a {
    display: inline-block;
    font-size: 20px;
    padding: 4px;
}

.author-row-new sup {
    color: #313436;
    font-size: 12px;
}

.affiliations-new {
    font-size: 18px;
    text-align: center;
    width: 80%;
    margin: 0 auto;
    margin-bottom: 20px;
}

.row {
    margin: 16px 0px 16px 0px;
}
.authors {
    font-size: 26px;
}
.affiliatons {
    font-size: 18px;
}
.affil-row {
    margin-top: 18px;
}
.teaser {
    max-width: 100%;
}
.text-center {
    text-align: center;  
}
.screenshot {
    width: 256px;
    border: 1px solid #ddd;
}
.screenshot-el {
    margin-bottom: 16px;
}
hr {
    height: 1px;
    border: 0; 
    border-top: 1px solid #ddd;
    margin: 0;
}
.material-icons {
    vertical-align: -6px;
}
p {
    line-height: 1.25em;
}
.caption {
    font-size: 16px;
    color: #666;
    margin-top: 4px;
    margin-bottom: 10px;
	text-align: left;
}


video {
    display: block;
    margin: auto;
}


figure {
    display: block;
    margin: auto;
    margin-top: 10px;
    margin-bottom: 10px;
}
#bibtex pre {
    font-size: 14px;
    background-color: #eee;
    padding: 16px;
}
.blue {
    color: #2c82c9;
    font-weight: bold;
}
.orange {
    color: #d35400;
    font-weight: bold;
}
.flex-row {
    display: flex;
    flex-flow: row wrap;
    padding: 0;
    margin: 0;
    list-style: none;
}

.paper-btn-coming-soon {
    position: relative; 
    top: 0;
    left: 0;
}

.coming-soon {
    position: absolute;
    top: -15px;
    right: -15px;
}

.center {
  margin-left: 10.0%;
  margin-right: 10.0%;
}

.paper-btn {
  position: relative;
  text-align: center;

  display: inline-block;
  margin: 8px;
  padding: 8px 8px;

  border-width: 0;
  outline: none;
  border-radius: 2px;
  
  background-color: #5364cc;
  color: white !important;
  font-size: 20px;
  width: 100px;
  font-weight: 600;
}
.paper-btn-parent {
    display: flex;
    justify-content: center;
    margin: 16px 0px;
}

.paper-btn:hover {
    opacity: 0.85;
}

.container {
    margin-left: auto;
    margin-right: auto;
    padding-left: 16px;
    padding-right: 16px;
}

.venue {
    font-size: 23px;
}

.topnav {
    background-color: #EEEEEE;
    overflow: hidden;
}

.topnav div {
    max-width: 1070px;
    margin: 0 auto;
}

.topnav a {
    display: inline-block;
    color: black;
    text-align: center;
    vertical-align: middle;
    padding: 16px 16px;
    text-decoration: none;
    font-size: 18px;
}

.topnav img {
    padding: 2px 0px;
    width: 100%;
    margin: 0.2em 0px 0.3em 0px;
    vertical-align: middle;
}

pre {
    font-size: 0.9em;
    padding-left: 7px;
    padding-right: 7px;
    padding-top: 3px;
    padding-bottom: 3px;
    border-radius: 3px;
    background-color: rgb(235, 235, 235);
    overflow-x: auto;
}

.download-thumb {
    display: flex;
}

@media only screen and (max-width: 620px) {
    .download-thumb {
        display: none;
    }
}

.paper-stuff {
    width: 50%;
    font-size: 20px;
}

@media only screen and (max-width: 620px) {
    .paper-stuff {
        width: 100%;
    }
}
* {
  box-sizing: border-box;
}

.column {
  text-align: center;
  float: left;
  width: 16.666%;
  padding: 5px;
}
.column3 {
  text-align: center;
  float: left;
  width: 33.333%;
  padding: 5px;
}
.column4 {
  text-align: center;
  float: left;
  width: 50%;
  padding: 5px;
}
.column5 {
  text-align: center;
  float: left;
  width: 20%;
  padding: 5px;
}
.column10 {
  text-align: center;
  float: left;
  width: 10%;
  padding: 5px;
}
.border-right {
    border-right: 1px solid black;
}
.border-bottom{
    border-bottom: 1px solid black;
}


.row-center {
    margin: 16px 0px 16px 0px;
    text-align: center;
}

/* Clearfix (clear floats) */
.row::after {
  content: "";
  clear: both;
  display: table;
}
.img-fluid {
  max-width: 100%;
  height: auto;
}
.figure-img {
  margin-bottom: 0.5rem;
  line-height: 1;
}








.rounded-circle {
  border-radius: 50% !important;
}






/* Responsive layout - makes the three columns stack on top of each other instead of next to each other */
@media screen and (max-width: 500px) {
  .column {
    width: 100%;
  }
}
@media screen and (max-width: 500px) {
  .column3 {
    width: 100%;
  }
}

</style>
<link rel="stylesheet" href="bootstrap-grid.css">

<script type="text/javascript" src="../js/hidebib.js"></script>
    <link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic' rel='stylesheet' type='text/css'>
    <head>
        <title> Robust Unposed Generalizable Neural Radiance Fields (RUGNeRF) </title>
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta property="og:description" content="Wide baseline novel view synthesis using learned priors"/>
        <link href="https://fonts.googleapis.com/css2?family=Material+Icons" rel="stylesheet">
        <meta name="twitter:card" content="summary_large_image">
        <meta name="twitter:creator" content="@du_yilun">
        <meta name="twitter:title" content="Learning to Render Novel Views from Wide-Baseline Stereo Pairs">
        <meta name="twitter:description" content="">
        <meta name="twitter:image" content="">
    </head>

<body>

<div class="container">
    <div class="paper-title">
    <h1> 
Robust Unposed Generalizable Neural Radiance Fields (RUGNeRF)
<img src="img/rug.jpg" width=50px/>
    </div>

    <div id="authors">
        <center>
            <div class="author-row-new">
                <a href="https://cameronosmith.github.io">Cameron Smith<sup>1</sup></a>,
                <a href="https://yilundu.github.io/">Yilun Du<sup>1</sup></a>,
                <a href="https://ayushtewari.com/">Ayush Tewari<sup>1</sup></a>,
                <a href="https://vsitzmann.github.io/">Vincent Sitzmann<sup>1</sup></a>
            </div>
        </center>
        <center>
        <div class="affiliations">
            <span><sup>1</sup> MIT</span>
        </div>

        <div class="affil-row">
            <div class="venue text-center"><b>CVPR 2023 </b></div>
        </div>

        </center>

        <div style="clear: both">
            <div class="paper-btn-parent">
            <a class="paper-btn" href="">
                <span class="material-icons"> description </span> 
                 Paper
            </a>
            <a class="paper-btn" href="">
                <span class="material-icons"> code </span> 
                 Colab
            </a>
            <a class="paper-btn" href="">
                <span class="material-icons"> code </span>
                Code
            </a>
            </div>
        </div>
    </div>

    <section id="teaser-image">
        <center>
            <figure>
                <video width="50%" playsinline="" controls autoplay="" loop="" preload="" muted="">
                    <source src="img/hydrant_overview.mp4" type="video/mp4">
                </video>
            </figure>

        </center>
    </section>

    <section id="abstract"/>
        <hr>
        <div class="flex-row">
            <p>
RUGNeRF (Robust Unposed Generalizable Neural Radiance Fields) proposes a self-supervised, feedforward camera estimator that actually works and enables large-scale scene representation learning on uncurated static videos. We demonstrate that it actually works by using it for CO3D, a rotation-dominant dataset which classical SFM often struggles with. We also show our trained scene representation can provide accurate initializations for single-scene pose-estimation optimization methods, etc etc 
            </p>
        </div>
    </section>


    <hr>
    <div class="section">
        <h2>Feedforward Results: Pose Estimation, Video Reconstruction, Novel View synthesis</h2>
        <hr>
    <div class="video-grid">
      <div class="video-row">
        <video src="img/re10k.mp4" playsinline="" controls autoplay="" loop="" preload="" muted=""></video>
        <video src="img/hydrant.mp4" playsinline="" controls autoplay="" loop="" preload="" muted=""></video>
      </div>
      <div class="video-row">
        <video src="img/10cat.mp4" playsinline="" controls autoplay="" loop="" preload="" muted=""></video>
        <video src="img/kitti.mp4" playsinline="" controls autoplay="" loop="" preload="" muted=""></video>
      </div>
    </div>
    </div>


    <div class="section">
        <h2>Using RUGNeRF for a near-accurate SLAM initialization</h2>
        <hr>
        <p>
            Our method generalizes surprisingly well to OOD scenes for pose estimation. 
            First we show this CO3D bike which it outside the training category distribution ( "hydrant","teddybear","apple", "ball", "bench", "cake", "donut", "plant", "suitcase", "vase").
            Next we globally fine tune the poses and model for 360 degree view synthesis and accurate poses.

            Video is the pose plot screenshot and then a video version of the view synthesis. 

            Next we show completely OOD SLAM estimation, on a casually captured video of a plaza. No intrinsics information is available. 
            We again show we have remarkbly good initialization on this completely OOD scene, and then in a short finetuning stage our poses and view synthesis result is 
            even higher quality and globally consistent (no drift).

            Same result but on this plaza scene.

        </p>
        <div class="row align-items-center">
            <img src="img/bike_fig.png" width="30%"/>
            <div class="col justify-content-center text-center">
                <video src="img/bike.mp4" playsinline="" controls autoplay="" loop="" preload="" muted="" width="70%"/>
            </div> 
        </div>
    </div>


    <div class="section">
        <h2>Limitations</h2>
        <hr>
        <p>
            We're limited to static scenes, still require intrinsics, too expensive, video has to be relatively low-framerate (can't be too large baseline between frames). 
        </p>
    </div>
    <div class="row align-items-center">
    </div>

    <div class="section">
        <h2>Paper</h2>
        <hr>
        <div>
            <div class="list-group">
                <a href=""
                   class="list-group-item">
                    <img src="img/paper_thumbnail.png" style="width:100%; margin-right:-20px; margin-top:-10px;">
                </a>
            </div>
        </div>
    </div>

        <div class="section">
            <h2>Our Related Projects</h2>
            <hr>
            <p>
                Check out our related projects on neural rendering and neural fields! <br>
            </p>


            <div class='row vspace-top'>
                <div class="col-sm-3">
                    <div class="move-down">
                        <img src='../nerflow/fig/gibson_full.gif' class='img-fluid'>
                    </div>
                </div>

                <div class="col">
                    <div class='paper-title'>
                        <a href="https://yilundu.github.io/nerflow/">Neural Radiance Flow for 4D View Synthesis and Video Processing</a>

                    </div>
                    <div>
                        We present a method to capture a dynamic scene utilizing a spatial-temporal radiance field. We enforce consistency in this field utilizing a continuous flow field. We show that such an approach enables us to synthesize novel views in dynamic scenes captured using as little as a single monocular video, and further show that our radiance field can be utilized to denoise and super-resolve input images.
                    </div>
                </div>
            </div>

            <div class='row vspace-top'>
                <div class="col-sm-3">
                    <img src='../data/projects/seeing3d.gif' class='img-fluid'>
                </div>

                <div class="col">
                    <div class='paper-title'>
                        <a href="https://prafullsharma.net/see3d/">Seeing 3D Objects in a Single Image via Self-Supervised Static-Dynamic Disentanglement</a>

                    </div>
                    <div>
                        We propose a method  that maps a single image of a scene to a 3D neural scene representation that captures and disentangles the movable and immovable parts of the scene. Each scene component is parameterized with 2D neural ground plans, which are grids of features aligned with the ground plane that can be locally decoded into 3D neural radiance fields. We learn ground plans self-supervised through neural rendering and demonstrate the widespread utility of such ground plans such as extraction of object-centric 3D representations, novel view synthesis, instance segmentation, and 3D bounding box prediction. 
                    </div>
                </div>
            </div>

		<div class='row vspace-top'>
		    <div class="col-sm-3">
			<img src='../data/projects/ndf.gif' class='img-fluid'>
		    </div>

		    <div class="col">
			<div class='paper-title'>
			    <a href="https://arxiv.org/abs/2112.05124">Neural Descriptor Fields: SE(3)-Equivariant Object Representations for Manipulation</a>
			</div>
			<div>
                We present Neural Descriptor Fields (NDFs), an SE3 equivariant object representation which enables manipulation of different categories of objects at arbitrary poses from a limited number (5-10) demonstrations.
            </div>
		    </div>
		</div>

            <div class='row vspace-top'>
                <div class="col-sm-3">
                    <img src='../ndf/img/audiovisual_manifold.gif' class='img-fluid'>
                </div>

                <div class="col">
                    <div class='paper-title'>
                        <a href="https://yilundu.github.io/gem/">Learning Signal-Agnostic Manifolds of Neural Fields</a>

                    </div>
                    <div>
                        We present a method to capture the underlying structure of arbitrary data signals by representing each point of data as a neural field. This enables us to interpolate and generate new samples in image, shape, audio, and audiovisual domains all using the same identical architecture.
                    </div>
                </div>
            </div>
    <hr>

    <!---
    <section id="bibtex">
        <h2>Citation</h2>
        <hr>
        <pre><code>@inproceedings{du2023cross,
                  title={Learning to Render Novel Views from Wide-Baseline Stereo Pairs},
                  author={Du, Yilun and Smith, Cameron and Tewari, Ayush and Sitzmann, Vincent},
                  booktitle={CVPR},
                  year={2023}
                }</code></pre>
    </section>
    --->

    <section>
        This webpage template was recycled from <a href='https://nv-tlabs.github.io/LION/'>here</a>.
        <center><p><a href='https://accessibility.mit.edu/'><b>Accessibility</b></a></p></center>
    </section>

</div>

</body>
</html>
